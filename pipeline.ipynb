{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c1ea95",
   "metadata": {},
   "source": [
    "# Full pipeline (Longformer + Bridging mechanism + BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LongformerModel, LongformerTokenizer, BartTokenizer, BartForConditionalGeneration, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import hashlib\n",
    "import jsonlines\n",
    "from rouge_score import rouge_scorer\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "\n",
    "# Download NLTK punkt tokenizer if not present\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Load Spacy model for sentence splitting\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm' model. This will happen only once.\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Factual Consistency Model (FactCC)\n",
    "print(\"Loading FactCC model from Hugging Face...\")\n",
    "factcc_model_path = 'manueldeprada/FactCC'\n",
    "factcc_tokenizer = AutoTokenizer.from_pretrained(factcc_model_path)\n",
    "factcc_model = AutoModelForSequenceClassification.from_pretrained(factcc_model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "factcc_model.eval()\n",
    "factcc_model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"FactCC model loaded successfully.\")\n",
    "\n",
    "# Helper Function for Sentence Splitting\n",
    "def split_sentences(text):\n",
    "    return [sent.text.strip() for sent in nlp(text).sents if sent.text.strip()]\n",
    "\n",
    "# Helper function to create overlapping document chunks\n",
    "def create_document_chunks(document, tokenizer, max_length=512, overlap=50):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    chunks = []\n",
    "    doc_max_length = max_length - 50\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + doc_max_length, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk_tokens))\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start += (doc_max_length - overlap)\n",
    "    return chunks\n",
    "\n",
    "# Key Sentence Extractor for Constraint Guidance\n",
    "class KeySentenceExtractor:\n",
    "    def __init__(self, extraction_percent=0.15, lambda_mmr=0.5):  # Adjusted parameters\n",
    "        self.extraction_percent = extraction_percent\n",
    "        self.lambda_mmr = lambda_mmr\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    def extract_key_sentences(self, text: str) -> list:\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return sentences\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(sentences)\n",
    "        similarities = cosine_similarity(tfidf_matrix)\n",
    "        sentence_scores = similarities.sum(axis=1)\n",
    "        num_to_extract = max(1, int(len(sentences) * self.extraction_percent))\n",
    "        num_to_extract = min(num_to_extract, len(sentences))\n",
    "        selected_indices = []\n",
    "        unselected_indices = list(range(len(sentences)))\n",
    "        selected_indices.append(unselected_indices.pop(np.argmax(sentence_scores)))\n",
    "        for _ in range(num_to_extract - 1):\n",
    "            if not unselected_indices:\n",
    "                break\n",
    "            mmr_scores = []\n",
    "            for i in unselected_indices:\n",
    "                relevance = sentence_scores[i]\n",
    "                diversity = max(similarities[i, j] for j in selected_indices) if selected_indices else 0\n",
    "                mmr = self.lambda_mmr * relevance - (1 - self.lambda_mmr) * diversity\n",
    "                mmr_scores.append(mmr)\n",
    "            next_idx = unselected_indices.pop(np.argmax(mmr_scores))\n",
    "            selected_indices.append(next_idx)\n",
    "        selected_indices.sort()\n",
    "        return [sentences[i] for i in selected_indices]\n",
    "\n",
    "# Constraint Extractor\n",
    "class ConstraintExtractor:\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def extract_constraints(self, sentences: list) -> list:\n",
    "        constraints = []\n",
    "        for sentence in sentences:\n",
    "            doc = self.nlp(sentence)\n",
    "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            numbers = [token.text for token in doc if token.like_num]\n",
    "            noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "            constraint = {\n",
    "                'sentence': sentence,\n",
    "                'entities': entities,\n",
    "                'numbers': numbers,\n",
    "                'noun_phrases': noun_phrases\n",
    "            }\n",
    "            constraints.append(constraint)\n",
    "        return constraints\n",
    "\n",
    "# Constrained Logits Processor\n",
    "class ConstrainedLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, constraint_token_ids: list, boost_factor: float = 1.5):  # Reduced boost_factor\n",
    "        self.constraint_token_ids = set(constraint_token_ids)\n",
    "        self.boost_factor = boost_factor\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        constraint_mask = torch.zeros_like(scores)\n",
    "        for token_id in self.constraint_token_ids:\n",
    "            if token_id < scores.shape[-1]:\n",
    "                constraint_mask[:, token_id] = 1\n",
    "        scores = scores + (constraint_mask * self.boost_factor)\n",
    "        return scores\n",
    "\n",
    "# Longformer Extractive Summarization Model\n",
    "class LongformerExtractiveSummarizationModel(nn.Module):\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(LongformerExtractiveSummarizationModel, self).__init__()\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(self.longformer.config.hidden_size, 1)\n",
    "        self.pos_weight = pos_weight if pos_weight is not None else torch.tensor(1.0)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, labels=None):\n",
    "        outputs = self.longformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.to(logits.device))\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Setup\n",
    "CHECKPOINT_PATH = \"./extractive_summarization_results/checkpoint-3148\"\n",
    "CHUNK_SIZE = 4096\n",
    "tokenizer_long = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model_bart.to(device)\n",
    "key_sentence_extractor = KeySentenceExtractor()\n",
    "constraint_extractor = ConstraintExtractor()\n",
    "\n",
    "# Coverage score\n",
    "def get_sentence_coverage(para1, para2, threshold=0.5):\n",
    "    sents1 = split_sentences(para1)\n",
    "    sents2 = split_sentences(para2)\n",
    "    emb1 = sent_model.encode(sents1, convert_to_tensor=True)\n",
    "    emb2 = sent_model.encode(sents2, convert_to_tensor=True)\n",
    "    matched = 0\n",
    "    for i in range(len(sents2)):\n",
    "        sims = util.cos_sim(emb2[i], emb1)[0]\n",
    "        if sims.max().item() >= threshold:\n",
    "            matched += 1\n",
    "    coverage = matched / len(sents2) if sents2 else 0.0\n",
    "    return round(coverage, 4)\n",
    "\n",
    "# Factual Consistency Evaluation Function with Sliding Window\n",
    "def calculate_factcc_score(original_document, summary):\n",
    "    summary_sentences = split_sentences(summary)\n",
    "    if not summary_sentences:\n",
    "        return 0.0\n",
    "    document_chunks = create_document_chunks(original_document, factcc_tokenizer)\n",
    "    consistency_scores = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in summary_sentences:\n",
    "            max_sentence_score = 0.0\n",
    "            for chunk in document_chunks:\n",
    "                inputs = factcc_tokenizer(chunk, sentence, return_tensors='pt', padding=True, truncation='only_first', max_length=512).to(device)\n",
    "                outputs = factcc_model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "                correct_score = probs[0][0].item()\n",
    "                if correct_score > max_sentence_score:\n",
    "                    max_sentence_score = correct_score\n",
    "            consistency_scores.append(max_sentence_score)\n",
    "    if not consistency_scores:\n",
    "        return 0.0\n",
    "    average_score = sum(consistency_scores) / len(consistency_scores)\n",
    "    return round(average_score, 4)\n",
    "\n",
    "# Load the Fine-tuned Model\n",
    "def load_model(checkpoint_path):\n",
    "    print(f\"Loading model from {checkpoint_path}...\")\n",
    "    model = LongformerExtractiveSummarizationModel()\n",
    "    model_file = None\n",
    "    if os.path.exists(os.path.join(checkpoint_path, \"pytorch_model.bin\")):\n",
    "        model_file = \"pytorch_model.bin\"\n",
    "    elif os.path.exists(os.path.join(checkpoint_path, \"model.safetensors\")):\n",
    "        model_file = \"model.safetensors\"\n",
    "    if not model_file:\n",
    "        raise FileNotFoundError(f\"No valid model file found in {checkpoint_path}. Expected 'pytorch_model.bin' or 'model.safetensors'.\")\n",
    "    if model_file.endswith(\".safetensors\"):\n",
    "        from safetensors.torch import load_file\n",
    "        state_dict = load_file(os.path.join(checkpoint_path, model_file))\n",
    "    else:\n",
    "        state_dict = torch.load(os.path.join(checkpoint_path, model_file), map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Generate Extractive Summary\n",
    "def generate_extractive_summary(document: str) -> str:\n",
    "    if not document or not document.strip():\n",
    "        return \"Input document is empty.\"\n",
    "    paragraphs = [p.strip() for p in document.split('\\n\\n') if p.strip()]\n",
    "    if not paragraphs:\n",
    "        document_sentences = sent_tokenize(document)\n",
    "        if not document_sentences:\n",
    "            return \"No sentences found in the document.\"\n",
    "    else:\n",
    "        document_sentences = [sent for para in paragraphs for sent in sent_tokenize(para)]\n",
    "    if not document_sentences:\n",
    "        return \"No sentences found in the document.\"\n",
    "   \n",
    "    top_k = max(1, int(len(document_sentences) * 0.1))\n",
    "   \n",
    "    sentence_embeddings = sent_model.encode(document_sentences, convert_to_numpy=True)\n",
    "    all_chunks_tokens = []\n",
    "    all_chunks_attention = []\n",
    "    all_chunks_global_attention = []\n",
    "    sentence_to_chunk_map = []\n",
    "    for para_idx, paragraph in enumerate(paragraphs):\n",
    "        current_input_ids = [tokenizer_long.cls_token_id]\n",
    "        current_attention_mask = [1]\n",
    "        current_sent_start_idx = len([s for p in paragraphs[:para_idx] for s in sent_tokenize(p)])\n",
    "        for sent_idx, sentence in enumerate(sent_tokenize(paragraph)):\n",
    "            global_sent_idx = current_sent_start_idx + sent_idx\n",
    "            sentence_tokens = tokenizer_long.encode(sentence, add_special_tokens=False)\n",
    "            if len(current_input_ids) + len(sentence_tokens) + 1 > CHUNK_SIZE:\n",
    "                padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "                current_input_ids += [tokenizer_long.pad_token_id] * padding_length\n",
    "                current_attention_mask += [0] * padding_length\n",
    "                global_attention_mask = [0] * CHUNK_SIZE\n",
    "                global_attention_mask[0] = 1\n",
    "                all_chunks_tokens.append(current_input_ids)\n",
    "                all_chunks_attention.append(current_attention_mask)\n",
    "                all_chunks_global_attention.append(global_attention_mask)\n",
    "                sentence_to_chunk_map.append((global_sent_idx, len(all_chunks_tokens) - 1))\n",
    "                current_input_ids = [tokenizer_long.cls_token_id]\n",
    "                current_attention_mask = [1]\n",
    "            current_input_ids += sentence_tokens\n",
    "            current_attention_mask += [1] * len(sentence_tokens)\n",
    "            sentence_to_chunk_map.append((global_sent_idx, len(all_chunks_tokens)))\n",
    "        if len(current_input_ids) > 1:\n",
    "            current_input_ids.append(tokenizer_long.sep_token_id)\n",
    "            current_attention_mask.append(1)\n",
    "            padding_length = CHUNK_SIZE - len(current_input_ids)\n",
    "            current_input_ids += [tokenizer_long.pad_token_id] * padding_length\n",
    "            current_attention_mask += [0] * padding_length\n",
    "            global_attention_mask = [0] * CHUNK_SIZE\n",
    "            global_attention_mask[0] = 1\n",
    "            all_chunks_tokens.append(current_input_ids)\n",
    "            all_chunks_attention.append(current_attention_mask)\n",
    "            all_chunks_global_attention.append(global_attention_mask)\n",
    "            sentence_to_chunk_map.append((current_sent_start_idx + len(sent_tokenize(paragraph)) - 1, len(all_chunks_tokens) - 1))\n",
    "    input_ids_tensor = torch.tensor(all_chunks_tokens)\n",
    "    attention_mask_tensor = torch.tensor(all_chunks_attention)\n",
    "    global_attention_mask_tensor = torch.tensor(all_chunks_global_attention)\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids_tensor,\n",
    "            attention_mask=attention_mask_tensor,\n",
    "            global_attention_mask=global_attention_mask_tensor\n",
    "        )\n",
    "    predictions = torch.sigmoid(logits)\n",
    "    aggregated_scores = []\n",
    "    for chunk, att_mask in zip(predictions, all_chunks_attention):\n",
    "        effective_len = sum(att_mask)\n",
    "        if effective_len > 2:\n",
    "            content_scores = chunk[1:effective_len - 1].tolist()\n",
    "            aggregated_scores.extend(content_scores)\n",
    "    sentence_scores = [0.0] * len(document_sentences)\n",
    "    for global_sent_idx, chunk_idx in sentence_to_chunk_map:\n",
    "        if global_sent_idx < len(document_sentences):\n",
    "            start_token = sum(len(tokenizer_long.encode(document_sentences[s], add_special_tokens=False)) for s in range(global_sent_idx))\n",
    "            end_token = start_token + len(tokenizer_long.encode(document_sentences[global_sent_idx], add_special_tokens=False))\n",
    "            if end_token <= len(aggregated_scores):\n",
    "                sentence_logits = aggregated_scores[start_token:end_token]\n",
    "                sentence_scores[global_sent_idx] = max(sentence_logits) if len(sentence_logits) > 0 else 0.0\n",
    "    selected_indices = np.argsort(sentence_scores)[-top_k:][::-1]\n",
    "    predicted_sentences = [document_sentences[i] for i in selected_indices]\n",
    "    return \" \".join(predicted_sentences)\n",
    "\n",
    "# Rephrase Text with Constraint-Guided BART\n",
    "def rephrase_text(input_text, original_text, boost_factor=1.5, extraction_percent=0.15):  # Added original_text, updated parameters\n",
    "    sentences = sent_tokenize(input_text)\n",
    "    input_count = len(sentences)\n",
    "    if input_count == 0:\n",
    "        return input_text\n",
    "    target_count = max(1, int(input_count * np.random.uniform(0.8, 0.9)))\n",
    "    rephrased_sentences = []\n",
    "    # Extract key sentences from both input_text and original_text\n",
    "    key_sentences = key_sentence_extractor.extract_key_sentences(input_text)\n",
    "    key_sentences.extend(key_sentence_extractor.extract_key_sentences(original_text))\n",
    "    key_sentences = list(set(key_sentences))  # Remove duplicates\n",
    "    constraints = constraint_extractor.extract_constraints(key_sentences)\n",
    "    constraint_texts = []\n",
    "    for constraint in constraints:\n",
    "        constraint_texts.extend([ent[0] for ent in constraint['entities']])\n",
    "        constraint_texts.extend(constraint['numbers'])\n",
    "        constraint_texts.extend(constraint['noun_phrases'])\n",
    "    constraint_texts = list(set(constraint_texts))\n",
    "    constraint_token_ids = []\n",
    "    for text in constraint_texts:\n",
    "        tokens = tokenizer_bart.encode(text, add_special_tokens=False)\n",
    "        constraint_token_ids.extend(tokens)\n",
    "    constraint_token_ids = list(set(constraint_token_ids))\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        ConstrainedLogitsProcessor(constraint_token_ids, boost_factor)\n",
    "    ])\n",
    "    for sentence in sentences[:target_count]:\n",
    "        input_tokens = tokenizer_bart.encode(sentence, add_special_tokens=False)\n",
    "        if not input_tokens:\n",
    "            continue\n",
    "        target_length = max(10, len(input_tokens) * 2)  # Dynamic max_length\n",
    "        min_length = max(5, len(input_tokens) // 2)    # Dynamic min_length\n",
    "        inputs = tokenizer_bart(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bart.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=target_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=8,  # Increased num_beams\n",
    "                logits_processor=logits_processor,\n",
    "                early_stopping=True\n",
    "                # Removed no_repeat_ngram_size\n",
    "            )\n",
    "        rephrased_sentence = tokenizer_bart.decode(outputs[0], skip_special_tokens=True)\n",
    "        rephrased_sentences.append(rephrased_sentence)\n",
    "    return \" \".join(rephrased_sentences)\n",
    "\n",
    "# Process Summary with Constraint-Guided BART\n",
    "def process_summary(summary, abstractive_summary, original_text):  # Added original_text\n",
    "    sentences = sent_tokenize(summary)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer_bart.encode(sentence, add_special_tokens=False)\n",
    "        if current_length + len(sentence_tokens) > CHUNK_SIZE:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = len(sentence_tokens)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += len(sentence_tokens)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    rephrased_chunks = [rephrase_text(chunk, original_text, boost_factor=1.5, extraction_percent=0.15) for chunk in chunks]\n",
    "    final_summary = \" \".join(rephrased_chunks)\n",
    "    return final_summary\n",
    "\n",
    "# Evaluation Metrics\n",
    "def calculate_metrics(original_text, reference, candidate):\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = rouge_scorer_obj.score(reference, candidate)\n",
    "    rouge = {\n",
    "        \"rouge1\": round(rouge_scores[\"rouge1\"].fmeasure, 4),\n",
    "        \"rouge2\": round(rouge_scores[\"rouge2\"].fmeasure, 4),\n",
    "        \"rougeL\": round(rouge_scores[\"rougeL\"].fmeasure, 4)\n",
    "    }\n",
    "    P, R, F1 = bert_score.score([candidate], [reference], lang=\"en\", verbose=False)\n",
    "    bert = {\n",
    "        \"bertscore_precision\": round(P[0].item(), 4),\n",
    "        \"bertscore_recall\": round(R[0].item(), 4),\n",
    "        \"bertscore_f1\": round(F1[0].item(), 4)\n",
    "    }\n",
    "    coverage = get_sentence_coverage(original_text, candidate)\n",
    "    factcc = calculate_factcc_score(original_text, candidate)\n",
    "    return {**rouge, **bert, \"coverage_score\": coverage, \"factcc_score\": factcc}\n",
    "\n",
    "# Main Execution with Test Data\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model = load_model(CHECKPOINT_PATH)\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        exit()\n",
    "    test_file = \"govreport_tfidf_vscode2/test.json\"\n",
    "    test_data = []\n",
    "    with jsonlines.open(test_file) as reader:\n",
    "        for i, item in enumerate(reader):\n",
    "            if i >= 100:\n",
    "                break\n",
    "            test_data.append(item)\n",
    "    all_metrics = []\n",
    "    for item in tqdm(test_data, desc=\"Processing samples\"):\n",
    "        original_text = item.get('original_text', '')\n",
    "        abstractive_summary = item.get('abstractive_summary', '')\n",
    "        if not original_text or not abstractive_summary:\n",
    "            continue\n",
    "        extractive_summary = generate_extractive_summary(original_text)\n",
    "        final_summary = process_summary(extractive_summary, abstractive_summary, original_text)  # Pass original_text\n",
    "        metrics = calculate_metrics(original_text, abstractive_summary, final_summary)\n",
    "        all_metrics.append(metrics)\n",
    "    if all_metrics:\n",
    "        avg_metrics = {key: round(sum(m[key] for m in all_metrics) / len(all_metrics), 4) for key in all_metrics[0]}\n",
    "        for metric, value in avg_metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "    else:\n",
    "        print(\"No valid summaries generated for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
